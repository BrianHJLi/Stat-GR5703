---
title: "STAT5703 HW2 Exercise 2"
author: "Wen Fan(wf2255), Banruo Xie(bx2168), Hanjun Li(hl3339)"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/', warning=FALSE, message=FALSE)
```

## Exercise 2.
#### Question 1.
```{r}
data <- read.table('scores.txt', header = TRUE)
```

```{r}
# Complete case analysis.
c1 <- cov(data,use="complete")
c1
```

```{r}
# Available case analysis.
c2 <- cov(data,use="pairwise")
c2
```

```{r}
# Mean imputation
data_mean=data
for(i in 1:ncol(data_mean)) {
  data_mean[ , i][is.na(data_mean[ , i])] <- mean(data_mean[ , i], na.rm = TRUE)
}
c3 <- cov(data_mean)
c3
```

```{r}
# Mean imputation with bootstrap
cov<-matrix(rep(0,25),ncol=5)
for(i in 1:400){
  sam<-sample(nrow(data),22,replace=TRUE)
  temp <- sapply(data[sam,], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
  cov <- cov + cov(temp)
}
c4 <- cov/400
c4
```

```{r,warning=FALSE}
# The EM-algorithm
library(Amelia)
Completed_data <- amelia(data,m=1,p2s=0)
c5 <- cov(Completed_data$imputations$imp1)
c5
```
Mean imputation and  Mean imputation with the bootstrap have smaller covariance than others. Only EM-algorithm has a negative covariance of x1 and x2.

#### Question 2.
By delta method, we can get $\sqrt{n}(\hat \lambda_1-\lambda_1)\to N(0,2\lambda_1^2)$, therefore asymptotic normality of $\hat\lambda_1$ is :$\hat\lambda_1\to N(\lambda_1,\frac{2\lambda_1^2}{n})$, the confidence interval of $\lambda_1$ is:
$$[\frac{\hat\lambda_1}{1+ z_{1-\alpha/2}\sqrt\frac{2}{n}},\frac{\hat\lambda_1}{1- z_{1-\alpha/2}\sqrt\frac{2}{n}}]$$
Because $\lambda_1$ is the largest eigenvalue of the population covariance matrix, we can get $\hat\lambda_1$ from each method and the intervals of $\lambda_1$:
```{r}
get_interval <- function(lambda) {
  n=nrow(data)
  print(paste0('[',lambda/(1+sqrt(2/n)*qnorm(0.975)),', ',lambda/(1-sqrt(2/n)*qnorm(0.975)),']'))}
get_interval(max(eigen(c1)$value))
get_interval(max(eigen(c2)$value))
get_interval(max(eigen(c3)$value))
get_interval(max(eigen(c4)$value))
get_interval(max(eigen(c5)$value))
```
Complete case analysis and available case analysis give us a higher covariance than Mean imputation but also have larger confidence intervals because our data only has few complete records.
The EM-algorithm generates a smaller range of confidence interval than Complete case  and available case but larger than mean imputation(with bootstrap or not) 
Therefore Mean imputation with the bootstrap might be a good method to handle missing data for this particular scores data.

#### Question 3.
```{r}
library(SMPracticals)
cov(mathmarks)
get_interval(max(eigen(cov(mathmarks))$value))
```
Using EM-algorithm generates a cloest confidence interval of $\lambda_1$ from the full data. Therefore the EM-algorithm might be the best method to fill in the missing data in this case which is not consistent with the result we thought at question2, because the data size in questions before is really small.

#### Question 4.

partially observed vectors:
$$X_i=\begin{bmatrix}
X_{io} \\
X_{im} \\
\end{bmatrix}$$
we have that, 
$$\mu^{(k)}=\begin{bmatrix}
\mu_{io}^{(k)} \\
\mu_{im}^{(k)} \\
\end{bmatrix}
,
\Sigma^{(k)}=\begin{bmatrix}
\Sigma_{ioo}^{(k)}&\Sigma_{iom}^{(k)} \\
\Sigma_{imo}^{(k)}&\Sigma_{imm}^{(k)} \\
\end{bmatrix}$$
Then, for E-step:
Because of
$$E(X_i|X_io)=\begin{bmatrix}
X_{io} \\
E(X_{im}|X_{io}) \\
\end{bmatrix}
$$
$$
E(X_iX_i^T|X_{io})=\begin{bmatrix}
X_{io}X_{io}^T&X_{io}E(X_{im}^T|X_{io}) \\
E(X_{im}|X_{io})X_{io}^T&E(X_{im}X_{im}^T|X_{io}) \\
\end{bmatrix}\\$$
where from the propertites of multivariate normal distribution,
$$
E(X_{im}|X_{io})=\mu_{im}^{(k)}+\Sigma_{imo}^{(k)}(\Sigma_{ioo}^{(k)})^{-1}(X_{io}-\mu_{io}^{(k)})\\$$
$$E(X_{im}X_{im}^T|X_{io})=Cov(X_{im}|X_{io})+E(X_{im}|X_{io})E(X_{im}|X_{io})^T\\=(\Sigma_{imm}^{(k)}-\Sigma_{imo}^{(k)}(\Sigma_{ioo}^{(k)})^{-1}\Sigma_{iom}^{(k)})+E(X_{im}|X_{io})E(X_{im}|X_{io})^T$$
Then, for M-step:
$$\mu^{(k+1)}:\frac{1}{n}\sum_{i=1}^nE(X_i|X_{io})=0,\ 
\Sigma^{(k+1)}:\frac{1}{n}\sum_{i=1}^nE(X_iX_i^T|X_{io})-\mu^{(k+1)}\mu^{(k+1)^T}=0$$
To simplify using the information above, we can get:
$$\mu^{(k+1)}:\sum_{i=1}^n(\hat{X_i}-\mu)=0,\ 
\Sigma^{(k+1)}:\sum_{i=1}^n(\Sigma-(\hat{X_i}-\mu)(\hat{X_i}-\mu)^T-C_i^{(k)})=0$$
